{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "y45f8cySYr28",
        "outputId": "cb4be160-8c9f-46cc-ed25-006785b35d83"
      },
      "outputs": [],
      "source": [
        "# US Traffic Crash Machine Learning\n",
        "# Cell 1: Workspace Environment Package Installation\n",
        "\n",
        "import sys\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "# Check if we're in a virtual environment\n",
        "def check_virtual_env():\n",
        "    \"\"\"Check if running in virtual environment\"\"\"\n",
        "    in_venv = hasattr(sys, 'real_prefix') or (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix)\n",
        "    \n",
        "    if in_venv:\n",
        "        print(f\"‚úì Running in virtual environment: {sys.prefix}\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  Using system Python: {sys.executable}\")\n",
        "        print(\"   Consider creating a virtual environment for this project.\")\n",
        "    \n",
        "    return in_venv\n",
        "\n",
        "# Install packages using the current Python environment\n",
        "def install_packages():\n",
        "    \"\"\"Install all required packages in current environment\"\"\"\n",
        "    \n",
        "    packages = [\n",
        "        # Core Data Science and Visualization\n",
        "        \"pandas\", \"numpy\", \"matplotlib\", \"seaborn\", \"plotly\",\n",
        "        \n",
        "        # Machine Learning Core\n",
        "        \"scikit-learn\", \"xgboost\",\n",
        "        \n",
        "        # Class Imbalance Handling\n",
        "        \"imbalanced-learn\",\n",
        "        \n",
        "        # Model Explainability\n",
        "        \"shap\", \"lime\",\n",
        "        \n",
        "        # Geospatial and Interactive Visualization\n",
        "        \"folium\",\n",
        "        \n",
        "        # Text Analysis\n",
        "        \"wordcloud\"\n",
        "    ]\n",
        "    \n",
        "    print(\"üì¶ Installing packages in workspace environment...\")\n",
        "    \n",
        "    for package in packages:\n",
        "        try:\n",
        "            print(f\"Installing {package}...\")\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", package])\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"‚ùå Failed to install {package}: {e}\")\n",
        "            return False\n",
        "    \n",
        "    return True\n",
        "\n",
        "# Check environment and install packages\n",
        "print(\"üîç Checking Python environment...\")\n",
        "is_venv = check_virtual_env()\n",
        "\n",
        "print(f\"\\nüêç Python executable: {sys.executable}\")\n",
        "print(f\"üìÅ Working directory: {os.getcwd()}\")\n",
        "\n",
        "# Install packages\n",
        "if install_packages():\n",
        "    print(\"\\n‚úÖ All packages installed successfully in workspace environment!\")\n",
        "    print(\"‚úÖ Ready for US Traffic Crash Analysis and Prediction\")\n",
        "else:\n",
        "    print(\"\\n‚ùå Some packages failed to install. Please check the output above.\")\n",
        "\n",
        "# Verify critical imports\n",
        "print(\"\\nüîç Verifying package installations...\")\n",
        "try:\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import sklearn\n",
        "    import xgboost\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    print(\"‚úÖ Core libraries verification successful\")\n",
        "    \n",
        "    # Show versions for key packages\n",
        "    print(f\"\\nüìã Package versions:\")\n",
        "    print(f\"   ‚Ä¢ pandas: {pd.__version__}\")\n",
        "    print(f\"   ‚Ä¢ numpy: {np.__version__}\")\n",
        "    print(f\"   ‚Ä¢ scikit-learn: {sklearn.__version__}\")\n",
        "    print(f\"   ‚Ä¢ xgboost: {xgboost.__version__}\")\n",
        "    \n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Import error: {e}\")\n",
        "    print(\"Please restart kernel and run this cell again.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlduuyPbYu4A",
        "outputId": "b55ba47a-f95a-4b8b-d903-ceb63e85d90d"
      },
      "outputs": [],
      "source": [
        "# Cell 2: System Check and XGBoost Version\n",
        "import xgboost\n",
        "print(f\"XGBoost Version: {xgboost.__version__}\")\n",
        "\n",
        "# GPU check (optional)\n",
        "import subprocess\n",
        "try:\n",
        "    result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "    if result.returncode == 0:\n",
        "        print(result.stdout)\n",
        "        print(\"GPU available\")\n",
        "    else:\n",
        "        print(\"No GPU or nvidia-smi not available\")\n",
        "except Exception as e:\n",
        "    print(\"No GPU or nvidia-smi not available\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HnjPT1YMzS0",
        "outputId": "256fa5bb-332a-4131-f592-33dcf3123f6f"
      },
      "outputs": [],
      "source": [
        "# Cell 3: Import Libraries and Configuration\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "# Machine Learning Libraries\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV, cross_validate\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import (accuracy_score, classification_report, f1_score,\n",
        "                             precision_score, recall_score,\n",
        "                             mean_absolute_error, mean_squared_error, r2_score)\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "\n",
        "# Models\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, HistGradientBoostingClassifier, HistGradientBoostingRegressor\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "\n",
        "# Sampling\n",
        "try:\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "    SMOTE_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"Warning: imbalanced-learn not installed. SMOTE will be skipped.\")\n",
        "    SMOTE_AVAILABLE = False\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Configuration\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette(\"colorblind\")\n",
        "\n",
        "print(\"‚úì All libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Helper Functions and Utilities\n",
        "\n",
        "class DataConfig:\n",
        "    \"\"\"Configuration class for data processing parameters\"\"\"\n",
        "    def __init__(self):\n",
        "        self.sample_size = 1000\n",
        "        self.test_size = 0.2\n",
        "        self.random_state = 42\n",
        "        self.cv_folds = 5\n",
        "        self.grid_search_cv_folds = 3\n",
        "        \n",
        "        # Feature definitions\n",
        "        self.numerical_features = [\n",
        "            'Temperature(F)', 'Wind_Chill(F)', 'Humidity(%)', \n",
        "            'Pressure(in)', 'Visibility(mi)', 'Wind_Speed(mph)', \n",
        "            'Precipitation(in)'\n",
        "        ]\n",
        "        \n",
        "        self.categorical_features = [\n",
        "            'County', 'State', 'Timezone', 'Wind_Direction',\n",
        "            'Weather_Condition', 'Sunrise_Sunset', 'Civil_Twilight',\n",
        "            'Nautical_Twilight', 'Astronomical_Twilight'\n",
        "        ]\n",
        "        \n",
        "        self.boolean_features = [\n",
        "            'Amenity', 'Bump', 'Crossing', 'Give_Way', 'Junction', \n",
        "            'No_Exit', 'Railway', 'Roundabout', 'Station', 'Stop', \n",
        "            'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop'\n",
        "        ]\n",
        "        \n",
        "        self.target_severity = 'Severity'\n",
        "        self.target_location = ['Start_Lat', 'Start_Lng']\n",
        "\n",
        "def print_step(step_name, step_num=None):\n",
        "    \"\"\"Print formatted step header\"\"\"\n",
        "    if step_num:\n",
        "        print(f\"\\n{'='*20} Step {step_num}: {step_name} {'='*20}\")\n",
        "    else:\n",
        "        print(f\"\\n{'='*20} {step_name} {'='*20}\")\n",
        "\n",
        "def print_info(message, indent=0):\n",
        "    \"\"\"Print formatted info message\"\"\"\n",
        "    prefix = \"  \" * indent + \"‚Üí \"\n",
        "    print(f\"{prefix}{message}\")\n",
        "\n",
        "def safe_file_load(file_path, sample_size=None, stratify_col=None):\n",
        "    \"\"\"Safely load and optionally sample data\"\"\"\n",
        "    try:\n",
        "        print_info(f\"Loading data from: {file_path}\")\n",
        "        start_time = time.time()\n",
        "        \n",
        "        df = pd.read_csv(file_path)\n",
        "        load_time = time.time() - start_time\n",
        "        \n",
        "        print_info(f\"Data loaded in {load_time:.2f} seconds\")\n",
        "        print_info(f\"Original dataset shape: {df.shape}\")\n",
        "        \n",
        "        # Apply sampling if specified\n",
        "        if sample_size and sample_size < len(df):\n",
        "            print_info(f\"Applying stratified sampling (n={sample_size})\")\n",
        "            \n",
        "            if stratify_col and stratify_col in df.columns:\n",
        "                df, _ = train_test_split(\n",
        "                    df, train_size=sample_size, \n",
        "                    stratify=df[stratify_col], \n",
        "                    random_state=42\n",
        "                )\n",
        "            else:\n",
        "                df = df.sample(n=sample_size, random_state=42)\n",
        "            \n",
        "            df = df.copy()\n",
        "            print_info(f\"Sampled dataset shape: {df.shape}\")\n",
        "        \n",
        "        return df, True\n",
        "        \n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå Error: File '{file_path}' not found\")\n",
        "        return None, False\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading data: {e}\")\n",
        "        return None, False\n",
        "\n",
        "def create_time_features(df, datetime_col='Start_Time'):\n",
        "    \"\"\"Create time-based features from datetime column\"\"\"\n",
        "    print_info(\"Creating time features\")\n",
        "    \n",
        "    # Convert to datetime\n",
        "    df[datetime_col] = pd.to_datetime(df[datetime_col], errors='coerce')\n",
        "    \n",
        "    # Extract time features\n",
        "    df['Hour'] = df[datetime_col].dt.hour\n",
        "    df['DayOfWeek'] = df[datetime_col].dt.dayofweek\n",
        "    df['Month'] = df[datetime_col].dt.month\n",
        "    df['IsWeekend'] = (df['DayOfWeek'] >= 5).astype(int)\n",
        "    \n",
        "    time_features = {\n",
        "        'numerical': ['Hour', 'Month'],\n",
        "        'categorical': ['DayOfWeek'],\n",
        "        'boolean': ['IsWeekend']\n",
        "    }\n",
        "    \n",
        "    print_info(f\"Added time features: {list(time_features['numerical'] + time_features['categorical'] + time_features['boolean'])}\")\n",
        "    return df, time_features\n",
        "\n",
        "def display_feature_summary(config, time_features=None):\n",
        "    \"\"\"Display comprehensive feature summary\"\"\"\n",
        "    all_numerical = config.numerical_features.copy()\n",
        "    all_categorical = config.categorical_features.copy()\n",
        "    all_boolean = config.boolean_features.copy()\n",
        "    \n",
        "    if time_features:\n",
        "        all_numerical.extend(time_features['numerical'])\n",
        "        all_categorical.extend(time_features['categorical'])\n",
        "        all_boolean.extend(time_features['boolean'])\n",
        "    \n",
        "    print_info(f\"Numerical features ({len(all_numerical)}): {all_numerical}\")\n",
        "    print_info(f\"Categorical features ({len(all_categorical)}): {all_categorical}\")\n",
        "    print_info(f\"Boolean features ({len(all_boolean)}): {all_boolean}\")\n",
        "    print_info(f\"Total features: {len(all_numerical + all_categorical + all_boolean)}\")\n",
        "    \n",
        "    return all_numerical, all_categorical, all_boolean\n",
        "\n",
        "print(\"‚úì Helper functions defined successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Data Loading and Initial Processing\n",
        "\n",
        "print_step(\"Data Loading and Initial Processing\", 1)\n",
        "\n",
        "# Initialize configuration\n",
        "config = DataConfig()\n",
        "\n",
        "# *** USER ATTENTION: UPDATE YOUR FILE PATH HERE ***\n",
        "file_path = 'US_Accidents_March23.csv'\n",
        "\n",
        "# Load and sample data\n",
        "df, load_success = safe_file_load(\n",
        "    file_path, \n",
        "    sample_size=config.sample_size, \n",
        "    stratify_col=config.target_severity\n",
        ")\n",
        "\n",
        "if not load_success:\n",
        "    raise Exception(\"Failed to load data. Please check the file path.\")\n",
        "\n",
        "# Store original columns for reference\n",
        "original_columns = df.columns.tolist()\n",
        "\n",
        "# Display severity distribution\n",
        "print_info(\"Severity class distribution:\")\n",
        "severity_dist = df[config.target_severity].value_counts(normalize=True) * 100\n",
        "for severity, pct in severity_dist.items():\n",
        "    print(f\"    Severity {severity}: {pct:.1f}%\")\n",
        "\n",
        "print(\"‚úì Data loading completed successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Feature Engineering and Selection\n",
        "\n",
        "print_step(\"Feature Engineering and Selection\", 2)\n",
        "\n",
        "# Clean data: Remove rows with missing target values\n",
        "print_info(\"Cleaning data - removing missing target values\")\n",
        "initial_shape = df.shape[0]\n",
        "\n",
        "# Convert Start_Time and handle missing values\n",
        "df, time_features = create_time_features(df)\n",
        "\n",
        "# Remove rows with missing values in critical columns\n",
        "critical_columns = [config.target_severity] + config.target_location + ['Start_Time']\n",
        "df.dropna(subset=critical_columns, inplace=True)\n",
        "\n",
        "final_shape = df.shape[0]\n",
        "print_info(f\"Removed {initial_shape - final_shape} rows with missing critical values\")\n",
        "print_info(f\"Final dataset shape: {df.shape}\")\n",
        "\n",
        "# Update feature lists with time features\n",
        "all_numerical, all_categorical, all_boolean = display_feature_summary(config, time_features)\n",
        "\n",
        "# Update config with time features\n",
        "config.numerical_features = all_numerical\n",
        "config.categorical_features = all_categorical\n",
        "config.boolean_features = all_boolean\n",
        "\n",
        "# Create feature matrix and targets\n",
        "all_features = all_numerical + all_categorical + all_boolean\n",
        "X = df[all_features].copy()\n",
        "y_severity = df[config.target_severity].copy()\n",
        "y_location = df[config.target_location].copy()\n",
        "\n",
        "# Process boolean features (fill NaN with False, convert to int)\n",
        "print_info(\"Processing boolean features\")\n",
        "for col in all_boolean:\n",
        "    if col in X.columns and X[col].isnull().any():\n",
        "        X[col] = X[col].fillna(False).astype(int)\n",
        "\n",
        "print_info(f\"Feature matrix shape: {X.shape}\")\n",
        "print_info(f\"Severity target shape: {y_severity.shape}\")\n",
        "print_info(f\"Location target shape: {y_location.shape}\")\n",
        "\n",
        "print(\"‚úì Feature engineering completed successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Data Preprocessing Pipeline\n",
        "\n",
        "print_step(\"Data Preprocessing Pipeline\", 3)\n",
        "\n",
        "def create_preprocessor(numerical_features, categorical_features, boolean_features):\n",
        "    \"\"\"Create and return a preprocessing pipeline\"\"\"\n",
        "    \n",
        "    # Numerical transformer\n",
        "    numerical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "    \n",
        "    # Categorical transformer\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "    ])\n",
        "    \n",
        "    # Combine transformers\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numerical_transformer, numerical_features),\n",
        "            ('cat', categorical_transformer, categorical_features),\n",
        "            ('bool', 'passthrough', boolean_features)\n",
        "        ],\n",
        "        remainder='drop',\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    \n",
        "    return preprocessor\n",
        "\n",
        "# Create preprocessor\n",
        "preprocessor = create_preprocessor(\n",
        "    config.numerical_features, \n",
        "    config.categorical_features, \n",
        "    config.boolean_features\n",
        ")\n",
        "\n",
        "print_info(\"Preprocessing pipeline created successfully\")\n",
        "print_info(f\"Numerical features: {len(config.numerical_features)}\")\n",
        "print_info(f\"Categorical features: {len(config.categorical_features)}\")\n",
        "print_info(f\"Boolean features: {len(config.boolean_features)}\")\n",
        "\n",
        "# Create train-test splits\n",
        "print_info(\"Creating train-test splits\")\n",
        "\n",
        "# Classification split (stratified)\n",
        "X_train_raw, X_test_raw, y_train_sev, y_test_sev = train_test_split(\n",
        "    X, y_severity, \n",
        "    test_size=config.test_size, \n",
        "    random_state=config.random_state, \n",
        "    stratify=y_severity\n",
        ")\n",
        "\n",
        "# Regression targets (using same indices)\n",
        "y_train_loc = y_location.loc[X_train_raw.index]\n",
        "y_test_loc = y_location.loc[X_test_raw.index]\n",
        "\n",
        "print_info(f\"Training set shape: {X_train_raw.shape}\")\n",
        "print_info(f\"Test set shape: {X_test_raw.shape}\")\n",
        "\n",
        "# Display split distributions\n",
        "print_info(\"Training set severity distribution:\")\n",
        "train_dist = y_train_sev.value_counts(normalize=True) * 100\n",
        "for severity, pct in train_dist.items():\n",
        "    print(f\"    Severity {severity}: {pct:.1f}%\")\n",
        "\n",
        "print(\"‚úì Data preprocessing pipeline setup completed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: Data Transformation and SMOTE Application\n",
        "\n",
        "print_step(\"Data Transformation and SMOTE Application\", 4)\n",
        "\n",
        "def apply_preprocessing(preprocessor, X_train, X_test):\n",
        "    \"\"\"Apply preprocessing to training and test data\"\"\"\n",
        "    print_info(\"Applying preprocessing transformations\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Fit and transform training data\n",
        "    X_train_processed = preprocessor.fit_transform(X_train)\n",
        "    \n",
        "    # Transform test data\n",
        "    X_test_processed = preprocessor.transform(X_test)\n",
        "    \n",
        "    processing_time = time.time() - start_time\n",
        "    print_info(f\"Preprocessing completed in {processing_time:.2f} seconds\")\n",
        "    print_info(f\"Processed training shape: {X_train_processed.shape}\")\n",
        "    print_info(f\"Processed test shape: {X_test_processed.shape}\")\n",
        "    \n",
        "    # Get feature names after preprocessing\n",
        "    try:\n",
        "        feature_names = preprocessor.get_feature_names_out()\n",
        "        print_info(f\"Total features after preprocessing: {len(feature_names)}\")\n",
        "        return X_train_processed, X_test_processed, feature_names\n",
        "    except:\n",
        "        print_info(\"Could not retrieve feature names from preprocessor\")\n",
        "        return X_train_processed, X_test_processed, None\n",
        "\n",
        "def apply_smote_sampling(X_train, y_train):\n",
        "    \"\"\"Apply SMOTE sampling to balance classes\"\"\"\n",
        "    if not SMOTE_AVAILABLE:\n",
        "        print_info(\"SMOTE not available, skipping resampling\")\n",
        "        return X_train, y_train\n",
        "    \n",
        "    print_info(\"Applying SMOTE sampling\")\n",
        "    \n",
        "    # Calculate appropriate k_neighbors\n",
        "    min_class_count = y_train.value_counts().min()\n",
        "    k_neighbors = min(5, max(1, min_class_count - 1))\n",
        "    \n",
        "    print_info(f\"Using k_neighbors={k_neighbors} for SMOTE\")\n",
        "    \n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        sampler = SMOTE(random_state=config.random_state, k_neighbors=k_neighbors)\n",
        "        X_resampled, y_resampled = sampler.fit_resample(X_train, y_train)\n",
        "        \n",
        "        sampling_time = time.time() - start_time\n",
        "        print_info(f\"SMOTE completed in {sampling_time:.2f} seconds\")\n",
        "        print_info(f\"Resampled training shape: {X_resampled.shape}\")\n",
        "        \n",
        "        # Display new distribution\n",
        "        print_info(\"Post-SMOTE severity distribution:\")\n",
        "        smote_dist = pd.Series(y_resampled).value_counts(normalize=True) * 100\n",
        "        for severity, pct in smote_dist.items():\n",
        "            print(f\"    Severity {severity}: {pct:.1f}%\")\n",
        "        \n",
        "        return X_resampled, y_resampled\n",
        "        \n",
        "    except Exception as e:\n",
        "        print_info(f\"SMOTE failed: {e}. Using original data.\")\n",
        "        return X_train, y_train\n",
        "\n",
        "# Apply preprocessing\n",
        "X_train_processed, X_test_processed, feature_names = apply_preprocessing(\n",
        "    preprocessor, X_train_raw, X_test_raw\n",
        ")\n",
        "\n",
        "# Apply SMOTE for classification\n",
        "X_train_sev_final, y_train_sev_final = apply_smote_sampling(\n",
        "    X_train_processed, y_train_sev\n",
        ")\n",
        "\n",
        "# For regression, use non-SMOTE data\n",
        "X_train_loc_final = X_train_processed\n",
        "y_train_loc_final = y_train_loc\n",
        "\n",
        "print(\"‚úì Data transformation and sampling completed successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 9: Model Definitions and Cross-Validation\n",
        "\n",
        "print_step(\"Model Selection and Cross-Validation\", 5)\n",
        "\n",
        "class ModelConfig:\n",
        "    \"\"\"Configuration for model selection and evaluation\"\"\"\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        self.cv = KFold(n_splits=config.cv_folds, shuffle=True, random_state=config.random_state)\n",
        "        self.cv_n_jobs = 1\n",
        "        self.grid_search_cv_folds = config.grid_search_cv_folds\n",
        "        self.grid_search_n_jobs = 1\n",
        "        \n",
        "        # Classification models\n",
        "        self.classifiers = {\n",
        "            \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=config.random_state, n_jobs=-1),\n",
        "            \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=config.random_state, n_jobs=-1),\n",
        "            \"XGBoost\": xgb.XGBClassifier(\n",
        "                objective='multi:softmax',\n",
        "                num_class=4,  # Will be updated dynamically\n",
        "                random_state=config.random_state,\n",
        "                n_jobs=-1,\n",
        "                tree_method='hist'\n",
        "            ),\n",
        "            \"Naive Bayes\": GaussianNB(),\n",
        "            \"HistGradientBoosting\": HistGradientBoostingClassifier(random_state=config.random_state),\n",
        "            \"KNN\": KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
        "        }\n",
        "        \n",
        "        # Regression models\n",
        "        self.regressors = {\n",
        "            \"Linear Regression\": LinearRegression(n_jobs=-1),\n",
        "            \"Ridge\": Ridge(random_state=config.random_state),\n",
        "            \"Random Forest Regressor\": RandomForestRegressor(n_estimators=100, random_state=config.random_state, n_jobs=-1),\n",
        "            \"XGBoost Regressor\": xgb.XGBRegressor(\n",
        "                objective='reg:squarederror', \n",
        "                random_state=config.random_state, \n",
        "                n_jobs=-1, \n",
        "                tree_method='hist'\n",
        "            ),\n",
        "            \"HistGradientBoosting Regressor\": HistGradientBoostingRegressor(random_state=config.random_state),\n",
        "            \"KNN Regressor\": KNeighborsRegressor(n_neighbors=5, n_jobs=-1)\n",
        "        }\n",
        "        \n",
        "        # Scoring metrics\n",
        "        self.clf_scoring_metrics = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro']\n",
        "        self.reg_scoring_metrics = ['r2', 'neg_mean_absolute_error']\n",
        "\n",
        "def run_classification_cv(models, X_train, y_train, model_config):\n",
        "    \"\"\"Run cross-validation for classification models\"\"\"\n",
        "    print_info(\"Running classification cross-validation\")\n",
        "    \n",
        "    # Update XGBoost num_class\n",
        "    unique_classes = len(np.unique(y_train))\n",
        "    if \"XGBoost\" in models:\n",
        "        models[\"XGBoost\"].set_params(num_class=unique_classes)\n",
        "    \n",
        "    results_mean = {}\n",
        "    results_std = {}\n",
        "    results_f1 = {}\n",
        "    \n",
        "    for name, model in models.items():\n",
        "        print_info(f\"Evaluating {name}\")\n",
        "        start_time = time.time()\n",
        "        \n",
        "        try:\n",
        "            # Prepare labels for XGBoost (0-indexed)\n",
        "            current_y = y_train - 1 if \"XGBoost\" in name and y_train.min() == 1 else y_train\n",
        "            \n",
        "            scores_dict = cross_validate(\n",
        "                model, X_train, current_y,\n",
        "                cv=model_config.cv, \n",
        "                scoring=model_config.clf_scoring_metrics,\n",
        "                n_jobs=model_config.cv_n_jobs, \n",
        "                return_train_score=False\n",
        "            )\n",
        "            \n",
        "            mean_scores = {metric: scores_dict[f'test_{metric}'].mean() for metric in model_config.clf_scoring_metrics}\n",
        "            std_scores = {metric: scores_dict[f'test_{metric}'].std() for metric in model_config.clf_scoring_metrics}\n",
        "            \n",
        "            results_mean[name] = mean_scores\n",
        "            results_std[name] = std_scores\n",
        "            results_f1[name] = mean_scores['f1_macro']\n",
        "            \n",
        "            elapsed = time.time() - start_time\n",
        "            print(f\"      {name}: F1-Macro = {mean_scores['f1_macro']:.4f} ({elapsed:.1f}s)\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"      ERROR with {name}: {e}\")\n",
        "            results_f1[name] = \"Error\"\n",
        "    \n",
        "    return results_mean, results_std, results_f1\n",
        "\n",
        "def run_regression_cv(models, X_train, y_train, model_config):\n",
        "    \"\"\"Run cross-validation for regression models\"\"\"\n",
        "    print_info(\"Running regression cross-validation\")\n",
        "    \n",
        "    results_mean = {}\n",
        "    results_std = {}\n",
        "    results_r2 = {}\n",
        "    \n",
        "    for name, model in models.items():\n",
        "        print_info(f\"Evaluating {name}\")\n",
        "        start_time = time.time()\n",
        "        \n",
        "        try:\n",
        "            multi_output_model = MultiOutputRegressor(model, n_jobs=1)\n",
        "            scores_dict = cross_validate(\n",
        "                multi_output_model, X_train, y_train,\n",
        "                cv=model_config.cv, \n",
        "                scoring=model_config.reg_scoring_metrics,\n",
        "                n_jobs=model_config.cv_n_jobs, \n",
        "                return_train_score=False\n",
        "            )\n",
        "            \n",
        "            mean_scores = {metric: scores_dict[f'test_{metric}'].mean() for metric in model_config.reg_scoring_metrics}\n",
        "            std_scores = {metric: scores_dict[f'test_{metric}'].std() for metric in model_config.reg_scoring_metrics}\n",
        "            \n",
        "            results_mean[name] = mean_scores\n",
        "            results_std[name] = std_scores\n",
        "            results_r2[name] = mean_scores['r2']\n",
        "            \n",
        "            elapsed = time.time() - start_time\n",
        "            mae = -mean_scores['neg_mean_absolute_error']\n",
        "            print(f\"      {name}: R¬≤ = {mean_scores['r2']:.4f}, MAE = {mae:.4f} ({elapsed:.1f}s)\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"      ERROR with {name}: {e}\")\n",
        "            results_r2[name] = \"Error\"\n",
        "    \n",
        "    return results_mean, results_std, results_r2\n",
        "\n",
        "# Initialize model configuration\n",
        "model_config = ModelConfig(config)\n",
        "\n",
        "print(\"‚úì Model definitions and CV functions ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 10: Classification Model Evaluation\n",
        "\n",
        "print_step(\"Classification Model Evaluation\", 6)\n",
        "\n",
        "# Run classification cross-validation\n",
        "clf_results_mean, clf_results_std, clf_results_f1 = run_classification_cv(\n",
        "    model_config.classifiers, \n",
        "    X_train_sev_final, \n",
        "    y_train_sev_final, \n",
        "    model_config\n",
        ")\n",
        "\n",
        "# Display results sorted by F1-macro score\n",
        "print_info(\"Classification Results (sorted by F1-Macro):\")\n",
        "valid_results = {k: v for k, v in clf_results_f1.items() if v != \"Error\"}\n",
        "\n",
        "if valid_results:\n",
        "    for name, score in sorted(valid_results.items(), key=lambda x: x[1], reverse=True):\n",
        "        print(f\"  {name}: F1-Macro = {score:.4f}\")\n",
        "    \n",
        "    # Select best classifier\n",
        "    best_classifier_name = max(valid_results.keys(), key=lambda k: valid_results[k])\n",
        "    best_classifier_score = valid_results[best_classifier_name]\n",
        "    \n",
        "    print_info(f\"Best Classifier: {best_classifier_name} (F1-Macro: {best_classifier_score:.4f})\")\n",
        "    \n",
        "    # Set model for fine-tuning (prefer XGBoost if available and performing well)\n",
        "    fine_tune_classifier_name = \"XGBoost\" if \"XGBoost\" in valid_results else best_classifier_name\n",
        "    print_info(f\"Selected for fine-tuning: {fine_tune_classifier_name}\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå No valid classification results obtained\")\n",
        "    fine_tune_classifier_name = None\n",
        "\n",
        "print(\"‚úì Classification evaluation completed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 11: Regression Model Evaluation\n",
        "\n",
        "print_step(\"Regression Model Evaluation\", 7)\n",
        "\n",
        "# Run regression cross-validation\n",
        "reg_results_mean, reg_results_std, reg_results_r2 = run_regression_cv(\n",
        "    model_config.regressors, \n",
        "    X_train_loc_final, \n",
        "    y_train_loc_final, \n",
        "    model_config\n",
        ")\n",
        "\n",
        "# Display results sorted by R¬≤ score\n",
        "print_info(\"Regression Results (sorted by R¬≤):\")\n",
        "valid_results = {k: v for k, v in reg_results_r2.items() if v != \"Error\"}\n",
        "\n",
        "if valid_results:\n",
        "    for name, score in sorted(valid_results.items(), key=lambda x: x[1], reverse=True):\n",
        "        mae_score_val = reg_results_mean[name]['neg_mean_absolute_error']\n",
        "        mae_score = -mae_score_val if mae_score_val != \"Error\" else \"N/A\"\n",
        "        mae_str = f\", MAE = {mae_score:.4f}\" if isinstance(mae_score, (int, float)) else \"\"\n",
        "        print(f\"  {name}: R¬≤ = {score:.4f}{mae_str}\")\n",
        "    \n",
        "    # Select best regressor\n",
        "    best_regressor_name = max(valid_results.keys(), key=lambda k: valid_results[k])\n",
        "    best_regressor_score = valid_results[best_regressor_name]\n",
        "    \n",
        "    print_info(f\"Best Regressor: {best_regressor_name} (R¬≤: {best_regressor_score:.4f})\")\n",
        "    \n",
        "    # Set model for fine-tuning (prefer XGBoost if available and performing well)\n",
        "    fine_tune_regressor_name = \"XGBoost Regressor\" if \"XGBoost Regressor\" in valid_results else best_regressor_name\n",
        "    print_info(f\"Selected for fine-tuning: {fine_tune_regressor_name}\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå No valid regression results obtained\")\n",
        "    fine_tune_regressor_name = None\n",
        "\n",
        "print(\"‚úì Regression evaluation completed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 12: Model Fine-Tuning\n",
        "\n",
        "print_step(\"Model Fine-Tuning\", 8)\n",
        "\n",
        "def fine_tune_xgb_classifier(X_train, y_train, model_config):\n",
        "    \"\"\"Fine-tune XGBoost classifier using GridSearch\"\"\"\n",
        "    print_info(\"Fine-tuning XGBoost Classifier\")\n",
        "    \n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 200],\n",
        "        'learning_rate': [0.1, 0.2],\n",
        "        'max_depth': [5, 7]\n",
        "    }\n",
        "    \n",
        "    xgb_clf = xgb.XGBClassifier(\n",
        "        objective='multi:softmax',\n",
        "        num_class=len(np.unique(y_train)),\n",
        "        random_state=config.random_state,\n",
        "        n_jobs=-1,\n",
        "        tree_method='hist'\n",
        "    )\n",
        "    \n",
        "    # Prepare labels for XGBoost (0-indexed)\n",
        "    y_train_xgb = y_train - 1 if y_train.min() == 1 else y_train\n",
        "    \n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=xgb_clf,\n",
        "        param_grid=param_grid,\n",
        "        scoring='f1_macro',\n",
        "        cv=model_config.grid_search_cv_folds,\n",
        "        n_jobs=model_config.grid_search_n_jobs,\n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "    start_time = time.time()\n",
        "    grid_search.fit(X_train, y_train_xgb)\n",
        "    \n",
        "    tuning_time = time.time() - start_time\n",
        "    print_info(f\"Grid search completed in {tuning_time:.2f} seconds\")\n",
        "    print_info(f\"Best parameters: {grid_search.best_params_}\")\n",
        "    print_info(f\"Best CV score (F1-Macro): {grid_search.best_score_:.4f}\")\n",
        "    \n",
        "    return grid_search.best_estimator_, grid_search.best_params_, grid_search.best_score_\n",
        "\n",
        "def fine_tune_xgb_regressor(X_train, y_train, model_config):\n",
        "    \"\"\"Fine-tune XGBoost regressor using GridSearch\"\"\"\n",
        "    print_info(\"Fine-tuning XGBoost Regressor\")\n",
        "    \n",
        "    param_grid = {\n",
        "        'estimator__n_estimators': [100, 200],\n",
        "        'estimator__learning_rate': [0.1, 0.2],\n",
        "        'estimator__max_depth': [5, 7]\n",
        "    }\n",
        "    \n",
        "    xgb_reg = xgb.XGBRegressor(\n",
        "        objective='reg:squarederror',\n",
        "        random_state=config.random_state,\n",
        "        n_jobs=-1,\n",
        "        tree_method='hist'\n",
        "    )\n",
        "    \n",
        "    multi_output_reg = MultiOutputRegressor(xgb_reg, n_jobs=1)\n",
        "    \n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=multi_output_reg,\n",
        "        param_grid=param_grid,\n",
        "        scoring='r2',\n",
        "        cv=model_config.grid_search_cv_folds,\n",
        "        n_jobs=model_config.grid_search_n_jobs,\n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "    start_time = time.time()\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    \n",
        "    tuning_time = time.time() - start_time\n",
        "    print_info(f\"Grid search completed in {tuning_time:.2f} seconds\")\n",
        "    print_info(f\"Best parameters: {grid_search.best_params_}\")\n",
        "    print_info(f\"Best CV score (R¬≤): {grid_search.best_score_:.4f}\")\n",
        "    \n",
        "    return grid_search.best_estimator_, grid_search.best_params_, grid_search.best_score_\n",
        "\n",
        "def train_default_model(model_name, models_dict, X_train, y_train, is_regression=False):\n",
        "    \"\"\"Train default model without fine-tuning\"\"\"\n",
        "    print_info(f\"Training default {model_name} model\")\n",
        "    \n",
        "    model = models_dict[model_name]\n",
        "    \n",
        "    if is_regression:\n",
        "        model = MultiOutputRegressor(model, n_jobs=1)\n",
        "    elif \"XGBoost\" in model_name and y_train.min() == 1:\n",
        "        # Adjust labels for XGBoost\n",
        "        y_train = y_train - 1\n",
        "    \n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "# Initialize final models\n",
        "final_clf_model = None\n",
        "final_reg_model = None\n",
        "clf_params = None\n",
        "reg_params = None\n",
        "clf_cv_score = None\n",
        "reg_cv_score = None\n",
        "\n",
        "# Fine-tune classification model\n",
        "if fine_tune_classifier_name == \"XGBoost\":\n",
        "    try:\n",
        "        final_clf_model, clf_params, clf_cv_score = fine_tune_xgb_classifier(\n",
        "            X_train_sev_final, y_train_sev_final, model_config\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print_info(f\"XGBoost fine-tuning failed: {e}\")\n",
        "        final_clf_model = train_default_model(\n",
        "            fine_tune_classifier_name, model_config.classifiers, \n",
        "            X_train_sev_final, y_train_sev_final\n",
        "        )\n",
        "elif fine_tune_classifier_name and fine_tune_classifier_name in model_config.classifiers:\n",
        "    final_clf_model = train_default_model(\n",
        "        fine_tune_classifier_name, model_config.classifiers,\n",
        "        X_train_sev_final, y_train_sev_final\n",
        "    )\n",
        "\n",
        "# Fine-tune regression model  \n",
        "if fine_tune_regressor_name == \"XGBoost Regressor\":\n",
        "    try:\n",
        "        final_reg_model, reg_params, reg_cv_score = fine_tune_xgb_regressor(\n",
        "            X_train_loc_final, y_train_loc_final, model_config\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print_info(f\"XGBoost regressor fine-tuning failed: {e}\")\n",
        "        final_reg_model = train_default_model(\n",
        "            fine_tune_regressor_name, model_config.regressors,\n",
        "            X_train_loc_final, y_train_loc_final, is_regression=True\n",
        "        )\n",
        "elif fine_tune_regressor_name and fine_tune_regressor_name in model_config.regressors:\n",
        "    final_reg_model = train_default_model(\n",
        "        fine_tune_regressor_name, model_config.regressors,\n",
        "        X_train_loc_final, y_train_loc_final, is_regression=True\n",
        "    )\n",
        "\n",
        "print(\"‚úì Model fine-tuning completed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 13: Final Model Evaluation on Test Set\n",
        "\n",
        "print_step(\"Final Model Evaluation on Test Set\", 9)\n",
        "\n",
        "def evaluate_classification_model(model, model_name, X_test, y_test, cv_params=None, cv_score=None):\n",
        "    \"\"\"Evaluate classification model on test set\"\"\"\n",
        "    print_info(f\"Evaluating {model_name} on test set\")\n",
        "    \n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        y_pred = model.predict(X_test)\n",
        "        prediction_time = time.time() - start_time\n",
        "        \n",
        "        # Adjust predictions for XGBoost if needed\n",
        "        if \"XGBoost\" in model_name and y_test.min() == 1:\n",
        "            y_pred = y_pred + 1\n",
        "        \n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        class_labels = sorted(y_test.unique())\n",
        "        target_names = [f\"Severity {i}\" for i in class_labels]\n",
        "        \n",
        "        report = classification_report(\n",
        "            y_test, y_pred, \n",
        "            labels=class_labels, \n",
        "            target_names=target_names, \n",
        "            zero_division=0\n",
        "        )\n",
        "        \n",
        "        print_info(f\"Prediction completed in {prediction_time:.2f} seconds\")\n",
        "        print_info(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "        \n",
        "        if cv_params:\n",
        "            print_info(f\"Fine-tuned parameters: {cv_params}\")\n",
        "        if cv_score:\n",
        "            print_info(f\"CV Score: {cv_score:.4f}\")\n",
        "        \n",
        "        print(\"\\\\nClassification Report:\")\n",
        "        print(report)\n",
        "        \n",
        "        return accuracy, report\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error evaluating classification model: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def evaluate_regression_model(model, model_name, X_test, y_test, cv_params=None, cv_score=None):\n",
        "    \"\"\"Evaluate regression model on test set\"\"\"\n",
        "    print_info(f\"Evaluating {model_name} on test set\")\n",
        "    \n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        y_pred = model.predict(X_test)\n",
        "        prediction_time = time.time() - start_time\n",
        "        \n",
        "        # Calculate metrics\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        rmse = np.sqrt(mse)\n",
        "        \n",
        "        # Individual target metrics\n",
        "        mae_lat = mean_absolute_error(y_test.iloc[:, 0], y_pred[:, 0])\n",
        "        mae_lng = mean_absolute_error(y_test.iloc[:, 1], y_pred[:, 1])\n",
        "        \n",
        "        print_info(f\"Prediction completed in {prediction_time:.2f} seconds\")\n",
        "        print_info(f\"Test R¬≤ Score: {r2:.4f}\")\n",
        "        print_info(f\"Test MAE (Overall): {mae:.4f}\")\n",
        "        print_info(f\"  - Latitude MAE: {mae_lat:.4f}\")\n",
        "        print_info(f\"  - Longitude MAE: {mae_lng:.4f}\")\n",
        "        print_info(f\"Test RMSE: {rmse:.4f}\")\n",
        "        \n",
        "        if cv_params:\n",
        "            print_info(f\"Fine-tuned parameters: {cv_params}\")\n",
        "        if cv_score:\n",
        "            print_info(f\"CV Score: {cv_score:.4f}\")\n",
        "        \n",
        "        return {\n",
        "            'r2': r2, 'mae': mae, 'rmse': rmse,\n",
        "            'mae_lat': mae_lat, 'mae_lng': mae_lng\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error evaluating regression model: {e}\")\n",
        "        return None\n",
        "\n",
        "# Evaluate classification model\n",
        "if final_clf_model:\n",
        "    clf_results = evaluate_classification_model(\n",
        "        final_clf_model, fine_tune_classifier_name, \n",
        "        X_test_processed, y_test_sev,\n",
        "        clf_params, clf_cv_score\n",
        "    )\n",
        "else:\n",
        "    print(\"‚ùå No classification model available for evaluation\")\n",
        "\n",
        "print()  # Add spacing\n",
        "\n",
        "# Evaluate regression model\n",
        "if final_reg_model:\n",
        "    reg_results = evaluate_regression_model(\n",
        "        final_reg_model, fine_tune_regressor_name,\n",
        "        X_test_processed, y_test_loc,\n",
        "        reg_params, reg_cv_score\n",
        "    )\n",
        "else:\n",
        "    print(\"‚ùå No regression model available for evaluation\")\n",
        "\n",
        "print(\"‚úì Final model evaluation completed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 14: Model Deployment Preparation\n",
        "\n",
        "print_step(\"Model Deployment Preparation\", 10)\n",
        "\n",
        "import pickle\n",
        "import joblib\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "def save_models_for_streamlit():\n",
        "    \"\"\"Save only essential models for Streamlit deployment\"\"\"\n",
        "    \n",
        "    # Create deployment directory if it doesn't exist\n",
        "    deployment_dir = \"streamlit_deployment\"\n",
        "    if not os.path.exists(deployment_dir):\n",
        "        os.makedirs(deployment_dir)\n",
        "        print_info(f\"Created deployment directory: {deployment_dir}\")\n",
        "    \n",
        "    saved_files = []\n",
        "    \n",
        "    # Save classification model\n",
        "    if final_clf_model:\n",
        "        clf_path = os.path.join(deployment_dir, \"severity_classifier.pkl\")\n",
        "        joblib.dump(final_clf_model, clf_path)\n",
        "        saved_files.append(\"severity_classifier.pkl\")\n",
        "        print_info(f\"‚úÖ Classification model saved: {clf_path}\")\n",
        "    \n",
        "    # Save regression model\n",
        "    if final_reg_model:\n",
        "        reg_path = os.path.join(deployment_dir, \"location_regressor.pkl\")\n",
        "        joblib.dump(final_reg_model, reg_path)\n",
        "        saved_files.append(\"location_regressor.pkl\")\n",
        "        print_info(f\"‚úÖ Regression model saved: {reg_path}\")\n",
        "    \n",
        "    # Save preprocessor\n",
        "    if 'preprocessor' in globals():\n",
        "        preprocessor_path = os.path.join(deployment_dir, \"preprocessor.pkl\")\n",
        "        joblib.dump(preprocessor, preprocessor_path)\n",
        "        saved_files.append(\"preprocessor.pkl\")\n",
        "        print_info(f\"‚úÖ Preprocessor saved: {preprocessor_path}\")\n",
        "    \n",
        "    # Save feature names\n",
        "    if 'feature_names' in globals() and feature_names is not None:\n",
        "        feature_names_path = os.path.join(deployment_dir, \"feature_names.pkl\")\n",
        "        joblib.dump(feature_names, feature_names_path)\n",
        "        saved_files.append(\"feature_names.pkl\")\n",
        "        print_info(f\"‚úÖ Feature names saved: {feature_names_path}\")\n",
        "    \n",
        "    # Create simple metadata for Streamlit\n",
        "    model_metadata = {\n",
        "        \"creation_date\": datetime.now().isoformat(),\n",
        "        \"project_name\": \"US Traffic Crash Analysis and Prediction\",\n",
        "        \"classification_model\": fine_tune_classifier_name if final_clf_model else None,\n",
        "        \"regression_model\": fine_tune_regressor_name if final_reg_model else None,\n",
        "        \"test_accuracy\": clf_results[0] if 'clf_results' in globals() and clf_results else None,\n",
        "        \"test_r2_score\": reg_results['r2'] if 'reg_results' in globals() and reg_results else None\n",
        "    }\n",
        "    \n",
        "    metadata_path = os.path.join(deployment_dir, \"model_metadata.json\")\n",
        "    with open(metadata_path, 'w') as f:\n",
        "        json.dump(model_metadata, f, indent=2)\n",
        "    saved_files.append(\"model_metadata.json\")\n",
        "    print_info(f\"‚úÖ Model metadata saved: {metadata_path}\")\n",
        "    \n",
        "    return deployment_dir, saved_files\n",
        "\n",
        "# Execute model saving for Streamlit\n",
        "if final_clf_model or final_reg_model:\n",
        "    print_info(\"üöÄ Saving models for Streamlit deployment...\")\n",
        "    \n",
        "    deployment_dir, saved_files = save_models_for_streamlit()\n",
        "    \n",
        "    # Display summary\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"üìä STREAMLIT DEPLOYMENT SUMMARY\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    if final_clf_model:\n",
        "        print(f\"‚úÖ Classification Model: {fine_tune_classifier_name}\")\n",
        "        if 'clf_results' in globals() and clf_results and clf_results[0]:\n",
        "            print(f\"   Test Accuracy: {clf_results[0]:.4f}\")\n",
        "    \n",
        "    if final_reg_model:\n",
        "        print(f\"‚úÖ Regression Model: {fine_tune_regressor_name}\")\n",
        "        if 'reg_results' in globals() and reg_results:\n",
        "            print(f\"   Test R¬≤ Score: {reg_results['r2']:.4f}\")\n",
        "    \n",
        "    print(f\"\\nüìÅ Files saved in '{deployment_dir}/' directory:\")\n",
        "    for file in saved_files:\n",
        "        print(f\"   ‚Ä¢ {file}\")\n",
        "    \n",
        "    print(\"\\nüéØ Ready for Streamlit deployment!\")\n",
        "    print(\"   Essential model files are saved and ready to use.\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå No trained models available for deployment\")\n",
        "\n",
        "print(\"‚úì Model deployment preparation completed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ny8tRBxXnnFx",
        "outputId": "944eb307-4956-4ce9-b213-69de3fdf1927"
      },
      "outputs": [],
      "source": [
        "# Cell 15: SHAP Analysis - Model Explainability\n",
        "\n",
        "print_step(\"SHAP Analysis - Model Explainability\", 10)\n",
        "\n",
        "try:\n",
        "    import shap\n",
        "    SHAP_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"‚ùå SHAP not available. Install with: pip install shap\")\n",
        "    SHAP_AVAILABLE = False\n",
        "\n",
        "if SHAP_AVAILABLE and final_clf_model:\n",
        "    \n",
        "    class SHAPConfig:\n",
        "        \"\"\"Configuration for SHAP analysis\"\"\"\n",
        "        def __init__(self):\n",
        "            self.num_samples = 500\n",
        "            self.num_features_display = 15\n",
        "            self.num_local_examples = 3\n",
        "    \n",
        "    def create_shap_explainer(model, X_background):\n",
        "        \"\"\"Create appropriate SHAP explainer based on model type\"\"\"\n",
        "        print_info(\"Creating SHAP explainer\")\n",
        "        \n",
        "        try:\n",
        "            # For tree-based models, use TreeExplainer\n",
        "            if hasattr(model, 'feature_importances_') or 'XGB' in str(type(model)):\n",
        "                explainer = shap.TreeExplainer(model)\n",
        "            else:\n",
        "                # For other models, use Explainer (general purpose)\n",
        "                explainer = shap.Explainer(model, X_background)\n",
        "            \n",
        "            print_info(\"SHAP explainer created successfully\")\n",
        "            return explainer\n",
        "            \n",
        "        except Exception as e:\n",
        "            print_info(f\"Error creating SHAP explainer: {e}\")\n",
        "            return None\n",
        "    \n",
        "    def generate_shap_summary_plot(shap_values, X_shap, title=\"SHAP Feature Importance\"):\n",
        "        \"\"\"Generate SHAP summary plot\"\"\"\n",
        "        try:\n",
        "            plt.figure(figsize=(12, 8))\n",
        "            shap.summary_plot(shap_values, X_shap, max_display=shap_config.num_features_display, show=False)\n",
        "            plt.title(title, fontsize=14)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "        except Exception as e:\n",
        "            print_info(f\"Error generating summary plot: {e}\")\n",
        "    \n",
        "    def generate_local_explanations(shap_values, X_shap, y_shap, model):\n",
        "        \"\"\"Generate local explanations using waterfall plots\"\"\"\n",
        "        print_info(\"Generating local explanations\")\n",
        "        \n",
        "        for i in range(min(shap_config.num_local_examples, len(X_shap))):\n",
        "            try:\n",
        "                true_class = y_shap.iloc[i] if hasattr(y_shap, 'iloc') else y_shap[i]\n",
        "                pred_class = int(model.predict([X_shap[i]])[0])\n",
        "                \n",
        "                # Adjust prediction for XGBoost if needed\n",
        "                if \"XGBoost\" in str(type(model)) and true_class >= 1:\n",
        "                    pred_class = pred_class + 1\n",
        "                \n",
        "                print_info(f\"Sample {i+1}: True={true_class}, Predicted={pred_class}\")\n",
        "                \n",
        "                # Create waterfall plot\n",
        "                if hasattr(shap_values, '__getitem__') and len(shap_values) > i:\n",
        "                    single_explanation = shap_values[i]\n",
        "                    \n",
        "                    # Handle multi-class case\n",
        "                    if hasattr(single_explanation, 'values') and len(single_explanation.values.shape) > 1:\n",
        "                        # Get prediction class index for XGBoost\n",
        "                        class_idx = pred_class - 1 if pred_class >= 1 else pred_class\n",
        "                        if class_idx < single_explanation.values.shape[1]:\n",
        "                            single_class_values = shap.Explanation(\n",
        "                                values=single_explanation.values[:, class_idx],\n",
        "                                base_values=single_explanation.base_values[class_idx],\n",
        "                                data=single_explanation.data,\n",
        "                                feature_names=getattr(single_explanation, 'feature_names', None)\n",
        "                            )\n",
        "                            \n",
        "                            plt.figure(figsize=(12, 8))\n",
        "                            shap.plots.waterfall(single_class_values, max_display=15, show=False)\n",
        "                            plt.title(f\"SHAP Explanation - Sample {i+1} (Class {pred_class})\", fontsize=13)\n",
        "                            plt.tight_layout()\n",
        "                            plt.show()\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print_info(f\"Error generating explanation for sample {i+1}: {e}\")\n",
        "    \n",
        "    # Initialize SHAP configuration\n",
        "    shap_config = SHAPConfig()\n",
        "    \n",
        "    # Prepare data for SHAP analysis\n",
        "    X_shap = X_test_processed[:shap_config.num_samples]\n",
        "    y_shap = y_test_sev.iloc[:shap_config.num_samples].reset_index(drop=True)\n",
        "    \n",
        "    print_info(f\"Analyzing {len(X_shap)} samples for SHAP explanation\")\n",
        "    \n",
        "    # Create explainer\n",
        "    explainer = create_shap_explainer(final_clf_model, X_train_sev_final)\n",
        "    \n",
        "    if explainer:\n",
        "        try:\n",
        "            print_info(\"Calculating SHAP values (this may take time)\")\n",
        "            shap_values = explainer(X_shap)\n",
        "            \n",
        "            # Generate global summary plot\n",
        "            print_info(\"Creating global feature importance plot\")\n",
        "            generate_shap_summary_plot(shap_values, X_shap, \"SHAP Global Feature Importance\")\n",
        "            \n",
        "            # Generate local explanations\n",
        "            generate_local_explanations(shap_values, X_shap, y_shap, final_clf_model)\n",
        "            \n",
        "            print(\"‚úì SHAP analysis completed successfully\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error in SHAP analysis: {e}\")\n",
        "    else:\n",
        "        print(\"‚ùå Could not create SHAP explainer\")\n",
        "\n",
        "else:\n",
        "    if not SHAP_AVAILABLE:\n",
        "        print(\"‚ùå SHAP analysis skipped: SHAP not installed\")\n",
        "    else:\n",
        "        print(\"‚ùå SHAP analysis skipped: No classification model available\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkewdPIPu1ei",
        "outputId": "e9b37f28-bfc8-46bd-9baf-cad439a9d3e3"
      },
      "outputs": [],
      "source": [
        "# Cell 16: Advanced XAI Analysis and Visualizations\n",
        "\n",
        "print_step(\"Advanced XAI Analysis and Visualizations\", 11)\n",
        "\n",
        "import os\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "class XAIConfig:\n",
        "    \"\"\"Configuration for XAI analysis\"\"\"\n",
        "    def __init__(self):\n",
        "        self.output_dir = \"xai_plots\"\n",
        "        self.max_features_display = 15\n",
        "        self.permutation_repeats = 5\n",
        "        self.figure_size = (12, 8)\n",
        "\n",
        "def setup_xai_environment():\n",
        "    \"\"\"Setup environment for XAI analysis\"\"\"\n",
        "    xai_config = XAIConfig()\n",
        "    \n",
        "    # Create output directory\n",
        "    if not os.path.exists(xai_config.output_dir):\n",
        "        os.makedirs(xai_config.output_dir)\n",
        "        print_info(f\"Created output directory: {xai_config.output_dir}\")\n",
        "    else:\n",
        "        print_info(f\"Using existing output directory: {xai_config.output_dir}\")\n",
        "    \n",
        "    # Set plotting style\n",
        "    plt.rcParams['figure.figsize'] = xai_config.figure_size\n",
        "    \n",
        "    return xai_config\n",
        "\n",
        "def get_output_path(filename, output_dir):\n",
        "    \"\"\"Get full path for output file\"\"\"\n",
        "    return os.path.join(output_dir, filename)\n",
        "\n",
        "def analyze_feature_importance(model, model_name, X_test, y_test, feature_names, output_dir, is_regression=False):\n",
        "    \"\"\"Analyze and visualize feature importance\"\"\"\n",
        "    print_info(f\"Analyzing feature importance for {model_name}\")\n",
        "    \n",
        "    try:\n",
        "        # Model-specific feature importance\n",
        "        if hasattr(model, 'feature_importances_'):\n",
        "            importances = model.feature_importances_\n",
        "            indices = np.argsort(importances)[-20:]  # Top 20\n",
        "            \n",
        "            plt.figure(figsize=(10, 8))\n",
        "            plt.title(f'Feature Importances - {model_name}')\n",
        "            plt.barh(range(len(indices)), importances[indices], color='steelblue', alpha=0.7)\n",
        "            \n",
        "            if feature_names is not None and len(feature_names) > max(indices):\n",
        "                plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
        "            else:\n",
        "                plt.yticks(range(len(indices)), [f\"feature_{i}\" for i in indices])\n",
        "            \n",
        "            plt.xlabel('Relative Importance')\n",
        "            plt.tight_layout()\n",
        "            \n",
        "            filename = f'{\"regression\" if is_regression else \"classification\"}_feature_importance.png'\n",
        "            plt.savefig(get_output_path(filename, output_dir))\n",
        "            plt.close()\n",
        "            print_info(f\"Feature importance plot saved: {filename}\")\n",
        "        \n",
        "        # Permutation importance\n",
        "        print_info(\"Calculating permutation importance\")\n",
        "        perm_importance = permutation_importance(\n",
        "            model, X_test, y_test,\n",
        "            n_repeats=5, random_state=42, n_jobs=1\n",
        "        )\n",
        "        \n",
        "        sorted_idx = perm_importance.importances_mean.argsort()[-15:]  # Top 15\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.title(f'Permutation Importance - {model_name}')\n",
        "        plt.barh(range(len(sorted_idx)), perm_importance.importances_mean[sorted_idx],\n",
        "                color='coral', alpha=0.7)\n",
        "        \n",
        "        if feature_names is not None and len(feature_names) > max(sorted_idx):\n",
        "            plt.yticks(range(len(sorted_idx)), [feature_names[i] for i in sorted_idx])\n",
        "        else:\n",
        "            plt.yticks(range(len(sorted_idx)), [f\"feature_{i}\" for i in sorted_idx])\n",
        "        \n",
        "        plt.xlabel('Decrease in Score')\n",
        "        plt.tight_layout()\n",
        "        \n",
        "        filename = f'{\"regression\" if is_regression else \"classification\"}_permutation_importance.png'\n",
        "        plt.savefig(get_output_path(filename, output_dir))\n",
        "        plt.close()\n",
        "        print_info(f\"Permutation importance plot saved: {filename}\")\n",
        "        \n",
        "        return perm_importance\n",
        "        \n",
        "    except Exception as e:\n",
        "        print_info(f\"Error in feature importance analysis: {e}\")\n",
        "        return None\n",
        "\n",
        "def analyze_subgroups(X_test_raw, X_test_processed, y_test, model, model_name, output_dir, is_regression=False):\n",
        "    \"\"\"Analyze model performance across different subgroups\"\"\"\n",
        "    print_info(\"Analyzing model performance across subgroups\")\n",
        "    \n",
        "    try:\n",
        "        # Day vs Night analysis\n",
        "        if 'Hour' in X_test_raw.columns:\n",
        "            print_info(\"Day vs Night analysis\")\n",
        "            \n",
        "            day_mask = X_test_raw['Hour'].between(6, 18)\n",
        "            night_mask = ~day_mask\n",
        "            \n",
        "            day_indices = np.where(day_mask)[0]\n",
        "            night_indices = np.where(night_mask)[0]\n",
        "            \n",
        "            if len(day_indices) > 0 and len(night_indices) > 0:\n",
        "                # Day performance\n",
        "                day_X = X_test_processed[day_indices]\n",
        "                day_y = y_test.iloc[day_indices] if hasattr(y_test, 'iloc') else y_test[day_indices]\n",
        "                day_pred = model.predict(day_X)\n",
        "                \n",
        "                # Night performance\n",
        "                night_X = X_test_processed[night_indices]\n",
        "                night_y = y_test.iloc[night_indices] if hasattr(y_test, 'iloc') else y_test[night_indices]\n",
        "                night_pred = model.predict(night_X)\n",
        "                \n",
        "                if is_regression:\n",
        "                    day_r2 = r2_score(day_y, day_pred)\n",
        "                    night_r2 = r2_score(night_y, night_pred)\n",
        "                    \n",
        "                    plt.figure(figsize=(10, 6))\n",
        "                    plt.bar(['Day', 'Night'], [day_r2, night_r2], color=['gold', 'navy'])\n",
        "                    plt.ylim(0, max(1, day_r2 * 1.1, night_r2 * 1.1))\n",
        "                    plt.title(f'R¬≤ Score: Day vs Night - {model_name}')\n",
        "                    plt.ylabel('R¬≤ Score')\n",
        "                    plt.grid(axis='y', alpha=0.3)\n",
        "                    \n",
        "                    print_info(f\"Day R¬≤: {day_r2:.4f}, Night R¬≤: {night_r2:.4f}\")\n",
        "                else:\n",
        "                    # Adjust predictions for XGBoost if needed\n",
        "                    if \"XGBoost\" in model_name and day_y.min() == 1:\n",
        "                        day_pred = day_pred + 1\n",
        "                        night_pred = night_pred + 1\n",
        "                    \n",
        "                    day_acc = accuracy_score(day_y, day_pred)\n",
        "                    night_acc = accuracy_score(night_y, night_pred)\n",
        "                    \n",
        "                    plt.figure(figsize=(10, 6))\n",
        "                    plt.bar(['Day', 'Night'], [day_acc, night_acc], color=['gold', 'navy'])\n",
        "                    plt.ylim(0, 1)\n",
        "                    plt.title(f'Accuracy: Day vs Night - {model_name}')\n",
        "                    plt.ylabel('Accuracy')\n",
        "                    plt.grid(axis='y', alpha=0.3)\n",
        "                    \n",
        "                    print_info(f\"Day accuracy: {day_acc:.4f}, Night accuracy: {night_acc:.4f}\")\n",
        "                \n",
        "                plt.tight_layout()\n",
        "                filename = f'{\"regression\" if is_regression else \"classification\"}_day_night_comparison.png'\n",
        "                plt.savefig(get_output_path(filename, output_dir))\n",
        "                plt.close()\n",
        "                print_info(f\"Day vs Night comparison saved: {filename}\")\n",
        "        \n",
        "        # Weekend vs Weekday analysis\n",
        "        if 'IsWeekend' in X_test_raw.columns or 'DayOfWeek' in X_test_raw.columns:\n",
        "            print_info(\"Weekend vs Weekday analysis\")\n",
        "            \n",
        "            if 'IsWeekend' in X_test_raw.columns:\n",
        "                weekend_mask = X_test_raw['IsWeekend'] == 1\n",
        "            else:\n",
        "                weekend_mask = X_test_raw['DayOfWeek'].isin([5, 6])\n",
        "            \n",
        "            weekday_mask = ~weekend_mask\n",
        "            \n",
        "            weekend_indices = np.where(weekend_mask)[0]\n",
        "            weekday_indices = np.where(weekday_mask)[0]\n",
        "            \n",
        "            if len(weekend_indices) > 0 and len(weekday_indices) > 0:\n",
        "                # Similar analysis as day/night but for weekend/weekday\n",
        "                weekend_X = X_test_processed[weekend_indices]\n",
        "                weekend_y = y_test.iloc[weekend_indices] if hasattr(y_test, 'iloc') else y_test[weekend_indices]\n",
        "                weekend_pred = model.predict(weekend_X)\n",
        "                \n",
        "                weekday_X = X_test_processed[weekday_indices]\n",
        "                weekday_y = y_test.iloc[weekday_indices] if hasattr(y_test, 'iloc') else y_test[weekday_indices]\n",
        "                weekday_pred = model.predict(weekday_X)\n",
        "                \n",
        "                if is_regression:\n",
        "                    weekend_r2 = r2_score(weekend_y, weekend_pred)\n",
        "                    weekday_r2 = r2_score(weekday_y, weekday_pred)\n",
        "                    metric_name = \"R¬≤ Score\"\n",
        "                    weekend_score, weekday_score = weekend_r2, weekday_r2\n",
        "                else:\n",
        "                    if \"XGBoost\" in model_name and weekend_y.min() == 1:\n",
        "                        weekend_pred = weekend_pred + 1\n",
        "                        weekday_pred = weekday_pred + 1\n",
        "                    \n",
        "                    weekend_score = accuracy_score(weekend_y, weekend_pred)\n",
        "                    weekday_score = accuracy_score(weekday_y, weekday_pred)\n",
        "                    metric_name = \"Accuracy\"\n",
        "                \n",
        "                plt.figure(figsize=(10, 6))\n",
        "                plt.bar(['Weekend', 'Weekday'], [weekend_score, weekday_score], color=['purple', 'teal'])\n",
        "                if not is_regression:\n",
        "                    plt.ylim(0, 1)\n",
        "                plt.title(f'{metric_name}: Weekend vs Weekday - {model_name}')\n",
        "                plt.ylabel(metric_name)\n",
        "                plt.grid(axis='y', alpha=0.3)\n",
        "                plt.tight_layout()\n",
        "                \n",
        "                filename = f'{\"regression\" if is_regression else \"classification\"}_weekend_weekday_comparison.png'\n",
        "                plt.savefig(get_output_path(filename, output_dir))\n",
        "                plt.close()\n",
        "                print_info(f\"Weekend vs Weekday comparison saved: {filename}\")\n",
        "                print_info(f\"Weekend {metric_name.lower()}: {weekend_score:.4f}, Weekday {metric_name.lower()}: {weekday_score:.4f}\")\n",
        "    \n",
        "    except Exception as e:\n",
        "        print_info(f\"Error in subgroup analysis: {e}\")\n",
        "\n",
        "# Initialize XAI environment\n",
        "xai_config = setup_xai_environment()\n",
        "\n",
        "# Analyze classification model\n",
        "if final_clf_model:\n",
        "    print_info(\"Analyzing classification model\")\n",
        "    \n",
        "    # Feature importance analysis\n",
        "    analyze_feature_importance(\n",
        "        final_clf_model, fine_tune_classifier_name,\n",
        "        X_test_processed, y_test_sev, feature_names, xai_config.output_dir\n",
        "    )\n",
        "    \n",
        "    # Subgroup analysis\n",
        "    analyze_subgroups(\n",
        "        X_test_raw, X_test_processed, y_test_sev,\n",
        "        final_clf_model, fine_tune_classifier_name, xai_config.output_dir\n",
        "    )\n",
        "\n",
        "# Analyze regression model\n",
        "if final_reg_model:\n",
        "    print_info(\"Analyzing regression model\")\n",
        "    \n",
        "    # Feature importance analysis\n",
        "    analyze_feature_importance(\n",
        "        final_reg_model, fine_tune_regressor_name,\n",
        "        X_test_processed, y_test_loc, feature_names, xai_config.output_dir, is_regression=True\n",
        "    )\n",
        "    \n",
        "    # Subgroup analysis\n",
        "    analyze_subgroups(\n",
        "        X_test_raw, X_test_processed, y_test_loc,\n",
        "        final_reg_model, fine_tune_regressor_name, xai_config.output_dir, is_regression=True\n",
        "    )\n",
        "\n",
        "print_info(f\"All XAI visualizations saved to '{xai_config.output_dir}' directory\")\n",
        "print(\"‚úì Advanced XAI analysis completed successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 17: Project Summary and Results\n",
        "\n",
        "print_step(\"Project Summary and Results\", 12)\n",
        "\n",
        "def generate_project_summary():\n",
        "    \"\"\"Generate comprehensive project summary\"\"\"\n",
        "    \n",
        "    print(\"üöó US TRAFFIC CRASH ANALYSIS AND PREDICTION PROJECT\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Data Summary\n",
        "    print(\"\\nüìä DATA SUMMARY:\")\n",
        "    print(f\"  ‚Ä¢ Original dataset size: {df.shape}\")\n",
        "    print(f\"  ‚Ä¢ Sample size used: {config.sample_size:,}\")\n",
        "    print(f\"  ‚Ä¢ Features processed: {len(config.numerical_features + config.categorical_features + config.boolean_features)}\")\n",
        "    print(f\"  ‚Ä¢ Training set: {X_train_raw.shape[0]:,} samples\")\n",
        "    print(f\"  ‚Ä¢ Test set: {X_test_raw.shape[0]:,} samples\")\n",
        "    \n",
        "    # Model Performance Summary\n",
        "    print(\"\\nüéØ MODEL PERFORMANCE:\")\n",
        "    \n",
        "    if final_clf_model:\n",
        "        print(f\"  ‚Ä¢ Classification Model: {fine_tune_classifier_name}\")\n",
        "        if clf_params:\n",
        "            print(f\"    - Fine-tuned with GridSearch\")\n",
        "            print(f\"    - Best CV Score (F1-Macro): {clf_cv_score:.4f}\")\n",
        "        if 'clf_results' in globals() and clf_results:\n",
        "            accuracy, _ = clf_results\n",
        "            if accuracy:\n",
        "                print(f\"    - Test Accuracy: {accuracy:.4f}\")\n",
        "    else:\n",
        "        print(\"  ‚Ä¢ Classification Model: Not available\")\n",
        "    \n",
        "    if final_reg_model:\n",
        "        print(f\"  ‚Ä¢ Regression Model: {fine_tune_regressor_name}\")\n",
        "        if reg_params:\n",
        "            print(f\"    - Fine-tuned with GridSearch\")\n",
        "            print(f\"    - Best CV Score (R¬≤): {reg_cv_score:.4f}\")\n",
        "        if 'reg_results' in globals() and reg_results:\n",
        "            print(f\"    - Test R¬≤ Score: {reg_results['r2']:.4f}\")\n",
        "            print(f\"    - Test MAE: {reg_results['mae']:.4f}\")\n",
        "    else:\n",
        "        print(\"  ‚Ä¢ Regression Model: Not available\")\n",
        "    \n",
        "    # Technical Features\n",
        "    print(\"\\nüîß TECHNICAL FEATURES:\")\n",
        "    print(\"  ‚Ä¢ ‚úì Stratified sampling for balanced dataset\")\n",
        "    if SMOTE_AVAILABLE:\n",
        "        print(\"  ‚Ä¢ ‚úì SMOTE applied for class imbalance handling\")\n",
        "    print(\"  ‚Ä¢ ‚úì Comprehensive preprocessing pipeline\")\n",
        "    print(\"  ‚Ä¢ ‚úì Time-based feature engineering\")\n",
        "    print(\"  ‚Ä¢ ‚úì Cross-validation model selection\")\n",
        "    print(\"  ‚Ä¢ ‚úì Hyperparameter tuning with GridSearch\")\n",
        "    if SHAP_AVAILABLE:\n",
        "        print(\"  ‚Ä¢ ‚úì SHAP explainability analysis\")\n",
        "    print(\"  ‚Ä¢ ‚úì Advanced XAI visualizations\")\n",
        "    print(\"  ‚Ä¢ ‚úì Subgroup performance analysis\")\n",
        "    \n",
        "    # Outputs Generated\n",
        "    print(\"\\nüìÅ OUTPUTS GENERATED:\")\n",
        "    if os.path.exists(\"xai_plots\"):\n",
        "        plot_files = [f for f in os.listdir(\"xai_plots\") if f.endswith('.png')]\n",
        "        print(f\"  ‚Ä¢ XAI Visualizations: {len(plot_files)} plots in 'xai_plots/' directory\")\n",
        "        if plot_files:\n",
        "            print(\"    - Feature importance plots\")\n",
        "            print(\"    - Permutation importance analysis\")\n",
        "            print(\"    - Subgroup performance comparisons\")\n",
        "            if SHAP_AVAILABLE:\n",
        "                print(\"    - SHAP explanability plots\")\n",
        "    \n",
        "    # Key Insights\n",
        "    print(\"\\nüí° KEY INSIGHTS:\")\n",
        "    print(\"  ‚Ä¢ Traffic crash severity can be predicted with machine learning\")\n",
        "    print(\"  ‚Ä¢ Accident location prediction provides valuable insights\")\n",
        "    print(\"  ‚Ä¢ Time-based features significantly improve model performance\")\n",
        "    print(\"  ‚Ä¢ Weather conditions and road infrastructure impact crash severity\")\n",
        "    print(\"  ‚Ä¢ Model performance varies between day/night and weekend/weekday\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"üéâ ANALYSIS COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"   All models trained, evaluated, and explained.\")\n",
        "    print(\"   Check the 'xai_plots' directory for visualizations.\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "def display_execution_summary():\n",
        "    \"\"\"Display execution time and resource summary\"\"\"\n",
        "    print(\"\\n‚è±Ô∏è EXECUTION SUMMARY:\")\n",
        "    \n",
        "    # Calculate total cells executed\n",
        "    total_cells = 16\n",
        "    print(f\"  ‚Ä¢ Total pipeline steps: {total_cells}\")\n",
        "    print(f\"  ‚Ä¢ Modular design with error handling\")\n",
        "    print(f\"  ‚Ä¢ Optimized for reproducibility (random_state={config.random_state})\")\n",
        "    \n",
        "    # Memory and processing notes\n",
        "    print(\"\\nüíæ RESOURCE OPTIMIZATION:\")\n",
        "    print(f\"  ‚Ä¢ Sample size: {config.sample_size:,} (adjustable)\")\n",
        "    print(\"  ‚Ä¢ Parallel processing enabled where possible\")\n",
        "    print(\"  ‚Ä¢ Memory-efficient preprocessing pipeline\")\n",
        "    print(\"  ‚Ä¢ Configurable parameters for different environments\")\n",
        "\n",
        "# Generate comprehensive summary\n",
        "generate_project_summary()\n",
        "display_execution_summary()\n",
        "\n",
        "print(\"\\n‚ú® Thank you for using the US Traffic Crash Analysis and Prediction Machine Learning Project!\")\n",
        "print(\"   This refactored version provides better modularity, error handling, and explainability.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
